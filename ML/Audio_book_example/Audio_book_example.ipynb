{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio_book_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0OBtUZDU36W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "8c8e2949-7fcd-4ae0-edf3-091f040a2fe8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4KiARRLVKxt"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "raw_csv_data= np.loadtxt('/content/drive/My Drive/Colab Notebooks/1.1 Audiobooks_data.csv.csv',delimiter = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByQabb9SVwc2"
      },
      "source": [
        "unscaled_inputs_all=raw_csv_data[:,1:-1]\n",
        "targets_all=raw_csv_data[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61n9ZdyrWT0f"
      },
      "source": [
        "Balance the datasheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "169SXYKxWO8c"
      },
      "source": [
        "num_one_target=int(np.sum(targets_all))\n",
        "zero_target_counter=0\n",
        "indices_to_remove=[]\n",
        "\n",
        "for i in range(targets_all.shape[0]):\n",
        "  if targets_all[i] == 0:\n",
        "    zero_target_counter+=1\n",
        "    if zero_target_counter>num_one_target:\n",
        "      indices_to_remove.append(i)\n",
        "\n",
        "unscaled_inputs_equal_priors=np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
        "targets_equal_priors=np.delete(targets_all,indices_to_remove,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYtwq2k9abKl"
      },
      "source": [
        "Standarise the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hkC8pjafgl"
      },
      "source": [
        "scaled_inputs=preprocessing.scale(unscaled_inputs_equal_priors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_qfMyHaob3_"
      },
      "source": [
        "Shuffling the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVjOY9UdojVN"
      },
      "source": [
        "shuffle_indices=np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffle_indices)\n",
        "\n",
        "shuffle_inputs=scaled_inputs[shuffle_indices]\n",
        "shuffel_targets=targets_equal_priors[shuffle_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htHPpQQip5Xz"
      },
      "source": [
        "### Split the data into train,validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xehnsXRonwT"
      },
      "source": [
        "sample_count=shuffle_inputs.shape[0]\n",
        "train_sample_count=int(0.8*sample_count)\n",
        "validation_sample_count=int(0.1*sample_count)\n",
        "test_sample_count=sample_count-train_sample_count-validation_sample_count\n",
        "train_inputs=shuffle_inputs[:train_sample_count]\n",
        "train_targets=shuffel_targets[:train_sample_count]\n",
        "\n",
        "validation_inputs=shuffle_inputs[train_sample_count:train_sample_count+validation_sample_count]\n",
        "validation_targets=shuffel_targets[train_sample_count:train_sample_count+validation_sample_count]\n",
        "\n",
        "test_inputs=shuffle_inputs[train_sample_count+validation_sample_count:]\n",
        "test_targets=shuffel_targets[train_sample_count+validation_sample_count:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo1DMZ3gvUoS"
      },
      "source": [
        "Save the file in .npz format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehdoaNo2u87-"
      },
      "source": [
        "np.savez('Audio_data_train',inputs=train_inputs,targets=train_targets)\n",
        "np.savez('Audio_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
        "np.savez('Audio_data_test',inputs=test_inputs,targets=train_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSqosR5tHeY8"
      },
      "source": [
        "Class for batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V77gUzpWwabG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#create a class that will do batching for the algorithm\n",
        "# this code is extremely reusable. You should just change Audiobook_Data_Reader everywhere in the code\n",
        "class Audiobook_Data_Reader():\n",
        "  #dataset is a mandatory argument, while batch_sizeis optional\n",
        "  #if you don't input batch_size it will automatically take it as None\n",
        "  def __init__(self,dataset,batch_size=None):\n",
        "\n",
        "    # The dataset that load in one of \"train\",\"validation\",\"test\".\n",
        "    npz=np.load('Audio_data_{0}.npz'.format(dataset))\n",
        "\n",
        "    #Two variable that take the value of inputs and the targets. Inputs are float and targets are integers\n",
        "    self.inputs, self.targets = npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
        "\n",
        "    # Counts the batch number, given the size you feed it later\n",
        "    # If the batch size is none we are either validating or testing, so we want to take the data in a single batch\n",
        "    if batch_size is None:\n",
        "      self.batch_size = self.inputs.shape[0]\n",
        "    else:\n",
        "      self.batch_size= batch_size\n",
        "    self.curr_batch=0\n",
        "    self.batch_count=self.inputs.shape[0] // self.batch_size\n",
        "\n",
        "\n",
        "    # A method whichh load the next batch\n",
        "  def __next__(self):\n",
        "    if self.curr_batch >= self.batch_count:\n",
        "      self.curr_batch = 0\n",
        "      raise StopIteration()\n",
        "\n",
        "    # You slice the dataset in batch and the next function load them  one after the other\n",
        "    batch_slice=slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
        "    inputs_batch = self.inputs[batch_slice]\n",
        "    targets_batch = self.targets[batch_slice]\n",
        "    self.curr_batch +=1 \n",
        "\n",
        "    #one hot encoded the targets. In this example it's a bit superflows since we have a 0/1 column\n",
        "    # classification task with more than one target column\n",
        "    classes_num=2\n",
        "    targets_one_hot=np.zeros((targets_batch.shape[0], classes_num))\n",
        "    targets_one_hot[range(targets_batch.shape[0]),targets_batch]=1\n",
        "    return inputs_batch, targets_one_hot\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PUD9YXLizLw"
      },
      "source": [
        "Machine Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-48Cd_bNi48Z"
      },
      "source": [
        "import tensorflow.compat.v1 as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YQ3e--iCzQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a66c00f7-165e-47d7-a78e-80c902c375c6"
      },
      "source": [
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 50\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,[None,input_size])\n",
        "targets = tf.placeholder(tf.int32,[None,output_size])\n",
        "\n",
        "weight_1 = tf.get_variable(\"weight_1\", [input_size,hidden_layer_size])\n",
        "biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
        "\n",
        "output_1= tf.nn.relu(tf.matmul(inputs,weight_1) + biases_1)\n",
        "\n",
        "weight_2 = tf.get_variable(\"weight_2\", [hidden_layer_size,hidden_layer_size])\n",
        "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
        "\n",
        "output_2= tf.nn.relu(tf.matmul(output_1,weight_2) + biases_2)\n",
        "\n",
        "weight_3 = tf.get_variable(\"weight_3\", [hidden_layer_size,output_size])\n",
        "biases_3 = tf.get_variable(\"biases_3\",[output_size])\n",
        "\n",
        "output=tf.matmul(output_2,weight_3) + biases_3\n",
        "\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=targets)\n",
        "\n",
        "mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=0.01).minimize(mean_loss)\n",
        "\n",
        "output_equal_targets=tf.equal(tf.arg_max(output,1),tf.arg_max(targets,1))\n",
        "\n",
        "accuray = tf.reduce_mean(tf.cast(output_equal_targets,tf.float32))\n",
        "\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "intializer = tf.global_variables_initializer()\n",
        "sess.run(intializer)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "max_epochs=50\n",
        "\n",
        "prev_validation_loss = 999999999\n",
        "\n",
        "train_data=Audiobook_Data_Reader('train',batch_size)\n",
        "validation_data=Audiobook_Data_Reader('validation',batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2TdgAGuEtQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4a4b84b5-d3a9-453d-d7a4-e7b997f687d6"
      },
      "source": [
        "for epoch_counter in range(max_epochs):\n",
        "  curr_epoch_loss=0\n",
        "\n",
        "  for inputs_data,target_data in train_data:\n",
        "    _,batch_loss = sess.run([optimize,mean_loss],\n",
        "                            feed_dict = {inputs: inputs_data, targets:target_data})\n",
        "    curr_epoch_loss+=batch_loss\n",
        "\n",
        "  curr_epoch_loss/=train_data.batch_count\n",
        "\n",
        "  validation_loss=0.\n",
        "  validation_accuracy=0.\n",
        "  for inputs_data, target_data in validation_data:\n",
        "    validation_loss,validation_accuracy = sess.run([mean_loss,accuray],\n",
        "                                                   feed_dict = {inputs: inputs_data, targets:target_data})\n",
        "    \n",
        "  print('Epoch='+str(epoch_counter+1)+\n",
        "        '.Training loss='+'{0:.3f}'.format(curr_epoch_loss)+\n",
        "        '. Validation loss='+'{0:.3f}'.format(validation_loss)+\n",
        "        '. Validation accuracy='+str(validation_accuracy*100)+\n",
        "        '%')\n",
        "  if validation_loss > prev_validation_loss:\n",
        "    break\n",
        "  prev_validation_loss=validation_loss\n",
        "\n",
        "print('End of Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch=1.Training loss=0.376. Validation loss=0.359. Validation accuracy=81.00000023841858%\n",
            "Epoch=2.Training loss=0.356. Validation loss=0.338. Validation accuracy=81.99999928474426%\n",
            "Epoch=3.Training loss=0.346. Validation loss=0.328. Validation accuracy=81.99999928474426%\n",
            "Epoch=4.Training loss=0.344. Validation loss=0.319. Validation accuracy=81.99999928474426%\n",
            "Epoch=5.Training loss=0.339. Validation loss=0.316. Validation accuracy=82.99999833106995%\n",
            "Epoch=6.Training loss=0.338. Validation loss=0.312. Validation accuracy=81.00000023841858%\n",
            "Epoch=7.Training loss=0.334. Validation loss=0.314. Validation accuracy=82.99999833106995%\n",
            "End of Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBymZGh4HEcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee806eea-110a-4f65-cb46-56487d368814"
      },
      "source": [
        "test_data=Audiobook_Data_Reader('test')\n",
        "\n",
        "for input_batch, target_batch in train_data:\n",
        "  test_accuracy=sess.run([accuray],\n",
        "                         feed_dict={inputs:input_batch, targets:target_batch})\n",
        "test_accuracy_percentage=test_accuracy[0] * 100\n",
        "print('Test accuracy:'+str(test_accuracy_percentage)+'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:75.99999904632568%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5sr_MwbbwdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "187d6673-a742-469d-a1c5-2366dcbf0289"
      },
      "source": [
        "print(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.76]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}